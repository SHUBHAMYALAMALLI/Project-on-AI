{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3abf32c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: arxiv in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: semanticscholar in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: pyalex in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (0.19)\n",
      "Requirement already satisfied: habanero in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: wbdata in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (2.3.5)\n",
      "Requirement already satisfied: requests in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from arxiv) (6.0.12)\n",
      "Requirement already satisfied: tenacity in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from semanticscholar) (9.1.2)\n",
      "Requirement already satisfied: httpx in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from semanticscholar) (0.28.1)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from semanticscholar) (1.6.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from pyalex) (2.6.0)\n",
      "Requirement already satisfied: packaging>=24.1 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from habanero) (25.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.2 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from habanero) (6.0.3)\n",
      "Requirement already satisfied: tqdm>=4.66.5 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from habanero) (4.67.1)\n",
      "Requirement already satisfied: appdirs<2,>=1.4 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from wbdata) (1.4.4)\n",
      "Requirement already satisfied: backoff<3,>=2.2.1 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from wbdata) (2.2.1)\n",
      "Requirement already satisfied: cachetools<6,>=5.3.2 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from wbdata) (5.5.2)\n",
      "Requirement already satisfied: dateparser<2,>=1.2.0 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from wbdata) (1.2.2)\n",
      "Requirement already satisfied: decorator<6,>=5.1.1 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from wbdata) (5.2.1)\n",
      "Requirement already satisfied: shelved-cache<0.4,>=0.3.1 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from wbdata) (0.3.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.5 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from wbdata) (0.9.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from sentence-transformers) (12.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from requests) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from requests) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from requests) (2025.11.12)\n",
      "Requirement already satisfied: regex>=2024.9.11 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from dateparser<2,>=1.2.0->wbdata) (2025.11.3)\n",
      "Requirement already satisfied: tzlocal>=0.2 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from dateparser<2,>=1.2.0->wbdata) (5.3.1)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from httpx->semanticscholar) (4.12.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from httpx->semanticscholar) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from httpcore==1.*->httpx->semanticscholar) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.12.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from tqdm>=4.66.5->habanero) (0.4.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shubhamvy\\desktop\\pseudo project (1)\\python environment\\.venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv semanticscholar pyalex habanero wbdata sentence-transformers joblib pandas numpy requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e77cafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ShubhamVY\\Desktop\\pseudo project (1)\\python environment\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import wbdata\n",
    "from pyalex import Works\n",
    "from habanero import Crossref\n",
    "from semanticscholar import SemanticScholar\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14617b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossref = Crossref()\n",
    "sch = SemanticScholar()\n",
    "\n",
    "def get_rnd_expenditure(country=\"USA\"):\n",
    "    \"\"\"Fetch latest R&D expenditure (% of GDP) from World Bank\"\"\"\n",
    "    try:\n",
    "        url = f\"https://api.worldbank.org/v2/country/{country}/indicator/GB.XPD.RSDV.GD.ZS?format=json\"\n",
    "        data = requests.get(url).json()\n",
    "        return float(data[1][0][\"value\"])\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def get_openalex_metadata(doi):\n",
    "    \"\"\"Fetch citations and fields from OpenAlex\"\"\"\n",
    "    try:\n",
    "        work = Works()[doi]\n",
    "        return {\n",
    "            \"citations\": work[\"cited_by_count\"],\n",
    "            \"fields\": [f[\"display_name\"] for f in work[\"concepts\"]],\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"citations\": np.nan, \"fields\": []}\n",
    "\n",
    "def get_crossref_data(doi):\n",
    "    \"\"\"Get publication year and journal info\"\"\"\n",
    "    try:\n",
    "        cr_data = crossref.works(ids=doi)\n",
    "        pub_date = cr_data[\"message\"][\"issued\"][\"date-parts\"][0][0]\n",
    "        journal = cr_data[\"message\"][\"container-title\"][0]\n",
    "        return {\"journal\": journal, \"pub_year\": pub_date}\n",
    "    except Exception:\n",
    "        return {\"journal\": None, \"pub_year\": None}\n",
    "\n",
    "def get_semanticscholar_citations(title):\n",
    "    \"\"\"Fetch citation count via Semantic Scholar\"\"\"\n",
    "    try:\n",
    "        paper = sch.search_paper(title)\n",
    "        if paper and len(paper) > 0:\n",
    "            return paper[0][\"citationCount\"]\n",
    "        return np.nan\n",
    "    except Exception:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f24399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Fetching papers for: quantum\n",
      "‚úÖ Collected 1000 papers for 'quantum'\n",
      "üîç Fetching papers for: superconductivity\n",
      "‚úÖ Collected 2000 papers for 'superconductivity'\n",
      "üîç Fetching papers for: semiconductor\n",
      "‚úÖ Collected 3000 papers for 'semiconductor'\n",
      "üìò Total papers: 3000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>summary</th>\n",
       "      <th>published</th>\n",
       "      <th>doi</th>\n",
       "      <th>pdf_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thermal State Simulation with Pauli and Majora...</td>\n",
       "      <td>We introduce a propagation-based approach to t...</td>\n",
       "      <td>2026-02-04 18:59:02+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>https://arxiv.org/pdf/2602.04878v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Epitaxial growth optimization, measurement and...</td>\n",
       "      <td>Interface roughness scattering is an important...</td>\n",
       "      <td>2026-02-04 18:57:43+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>https://arxiv.org/pdf/2602.04874v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Requirements for Teleportation in an Intercity...</td>\n",
       "      <td>We investigate the hardware requirements for q...</td>\n",
       "      <td>2026-02-04 18:56:48+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>https://arxiv.org/pdf/2602.04869v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From Evaluation to Design: Using Potential Ene...</td>\n",
       "      <td>Machine Learning Interatomic Potentials (MLIPs...</td>\n",
       "      <td>2026-02-04 18:50:10+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>https://arxiv.org/pdf/2602.04861v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Digital signatures with classical shadows on n...</td>\n",
       "      <td>Quantum mechanics provides cryptographic primi...</td>\n",
       "      <td>2026-02-04 18:48:12+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>https://arxiv.org/pdf/2602.04859v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Thermal State Simulation with Pauli and Majora...   \n",
       "1  Epitaxial growth optimization, measurement and...   \n",
       "2  Requirements for Teleportation in an Intercity...   \n",
       "3  From Evaluation to Design: Using Potential Ene...   \n",
       "4  Digital signatures with classical shadows on n...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  We introduce a propagation-based approach to t...   \n",
       "1  Interface roughness scattering is an important...   \n",
       "2  We investigate the hardware requirements for q...   \n",
       "3  Machine Learning Interatomic Potentials (MLIPs...   \n",
       "4  Quantum mechanics provides cryptographic primi...   \n",
       "\n",
       "                  published   doi                             pdf_url  \n",
       "0 2026-02-04 18:59:02+00:00  None  https://arxiv.org/pdf/2602.04878v1  \n",
       "1 2026-02-04 18:57:43+00:00  None  https://arxiv.org/pdf/2602.04874v1  \n",
       "2 2026-02-04 18:56:48+00:00  None  https://arxiv.org/pdf/2602.04869v1  \n",
       "3 2026-02-04 18:50:10+00:00  None  https://arxiv.org/pdf/2602.04861v1  \n",
       "4 2026-02-04 18:48:12+00:00  None  https://arxiv.org/pdf/2602.04859v1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = arxiv.Client()\n",
    "topics = [\"quantum\", \"superconductivity\", \"semiconductor\"]\n",
    "\n",
    "papers = []\n",
    "for topic in topics:\n",
    "    print(f\"üîç Fetching papers for: {topic}\")\n",
    "    search = arxiv.Search(\n",
    "        query=topic,\n",
    "        max_results=1000,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    try:\n",
    "        for result in client.results(search):\n",
    "            papers.append({\n",
    "                \"title\": result.title,\n",
    "                \"summary\": result.summary,\n",
    "                \"published\": result.published,\n",
    "                \"doi\": result.doi,\n",
    "                \"pdf_url\": result.pdf_url\n",
    "            })\n",
    "        print(f\"‚úÖ Collected {len(papers)} papers for '{topic}'\")\n",
    "    except arxiv.UnexpectedEmptyPageError:\n",
    "        print(f\"‚ö†Ô∏è No more pages for {topic}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    time.sleep(2)\n",
    "\n",
    "df = pd.DataFrame(papers)\n",
    "print(f\"üìò Total papers: {len(df)}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8f2281a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Enriching paper metadata (parallel mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Papers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [36:25<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Metadata enrichment complete in parallel mode!\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm   # ‚úÖ Use normal tqdm (NOT notebook)\n",
    "\n",
    "CACHE_FILE = \"paper_cache.json\"\n",
    "\n",
    "# Load previous cache if it exists\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    with open(CACHE_FILE, \"r\") as f:\n",
    "        cache = json.load(f)\n",
    "else:\n",
    "    cache = {}\n",
    "\n",
    "\n",
    "def safe_call(func, *args, **kwargs):\n",
    "    \"\"\"Wrapper to safely call APIs with fallback.\"\"\"\n",
    "    try:\n",
    "        return func(*args, **kwargs)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def enrich_paper(row):\n",
    "\n",
    "    doi = str(row.get(\"doi\", \"\"))\n",
    "    title = row.get(\"title\", \"\")\n",
    "\n",
    "    # Use cached data if available\n",
    "    if doi in cache:\n",
    "        return cache[doi]\n",
    "\n",
    "    crossref_meta = safe_call(get_crossref_data, doi)\n",
    "    openalex_meta = safe_call(get_openalex_metadata, doi)\n",
    "    rnd = safe_call(get_rnd_expenditure, \"USA\")\n",
    "    citations = safe_call(get_semanticscholar_citations, title)\n",
    "\n",
    "    data = {\n",
    "        \"journal\": crossref_meta.get(\"journal\"),\n",
    "        \"pub_year\": crossref_meta.get(\"pub_year\"),\n",
    "        \"fields\": \", \".join(openalex_meta.get(\"fields\", [])),\n",
    "        \"citations\": citations or openalex_meta.get(\"citations\"),\n",
    "        \"rnd_gdp\": rnd,\n",
    "    }\n",
    "\n",
    "    cache[doi] = data\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"‚öôÔ∏è Enriching paper metadata (parallel mode)...\")\n",
    "\n",
    "extra_info = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "\n",
    "    futures = [\n",
    "        executor.submit(enrich_paper, row)\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    for future in tqdm(\n",
    "        concurrent.futures.as_completed(futures),\n",
    "        total=len(futures),\n",
    "        desc=\"Processing Papers\"\n",
    "    ):\n",
    "        try:\n",
    "            extra_info.append(future.result())\n",
    "        except Exception as e:\n",
    "            print(\"‚ö†Ô∏è Skipping one paper:\", e)\n",
    "\n",
    "\n",
    "# Save cache\n",
    "with open(CACHE_FILE, \"w\") as f:\n",
    "    json.dump(cache, f, indent=2)\n",
    "\n",
    "\n",
    "meta_df = pd.DataFrame(extra_info)\n",
    "\n",
    "df = pd.concat([df, meta_df], axis=1)\n",
    "\n",
    "df.to_csv(\"physics_papers_enriched_fast.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Metadata enrichment complete in parallel mode!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6da98758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 94/94 [02:12<00:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Feature matrix shape: (3000, 387)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"üî¢ Generating embeddings...\")\n",
    "embeddings = embedder.encode(df[\"summary\"].tolist(), show_progress_bar=True)\n",
    "\n",
    "# Combine numerical features\n",
    "numeric_features = np.array([\n",
    "    df[\"citations\"].fillna(0).values,\n",
    "    df[\"pub_year\"].fillna(0).values,\n",
    "    df[\"rnd_gdp\"].fillna(0).values\n",
    "]).T\n",
    "\n",
    "X = np.hstack((embeddings, numeric_features))\n",
    "print(\"‚úÖ Feature matrix shape:\", X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d240cacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Impact labels successfully created.\n",
      "impact_label\n",
      "Low       2999\n",
      "High         1\n",
      "Medium       0\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ensure citations column exists and has valid numeric values\n",
    "if \"citations\" not in df.columns:\n",
    "    df[\"citations\"] = 0\n",
    "\n",
    "# Convert to numeric (in case some non-numeric values slipped in)\n",
    "df[\"citations\"] = pd.to_numeric(df[\"citations\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "# Create robust impact label\n",
    "if \"impact_label\" not in df.columns:\n",
    "    unique_vals = df[\"citations\"].nunique()\n",
    "\n",
    "    # Case 1: At least 3 unique citation values ‚Üí 3 groups\n",
    "    if unique_vals >= 3:\n",
    "        try:\n",
    "            df[\"impact_label\"] = pd.qcut(\n",
    "                df[\"citations\"],\n",
    "                q=3,\n",
    "                labels=[\"Low\", \"Medium\", \"High\"],\n",
    "                duplicates=\"drop\"\n",
    "            )\n",
    "        except ValueError:\n",
    "            # fallback if bins collapse due to duplicates\n",
    "            df[\"impact_label\"] = pd.cut(\n",
    "                df[\"citations\"],\n",
    "                bins=3,\n",
    "                labels=[\"Low\", \"Medium\", \"High\"]\n",
    "            )\n",
    "\n",
    "    # Case 2: Exactly 2 unique values ‚Üí 2 groups\n",
    "    elif unique_vals == 2:\n",
    "        df[\"impact_label\"] = pd.cut(\n",
    "            df[\"citations\"],\n",
    "            bins=2,\n",
    "            labels=[\"Low\", \"High\"]\n",
    "        )\n",
    "\n",
    "    # Case 3: All citation values identical ‚Üí default label\n",
    "    else:\n",
    "        df[\"impact_label\"] = \"Low\"\n",
    "\n",
    "print(\"‚úÖ Impact labels successfully created.\")\n",
    "print(df[\"impact_label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a41519f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model retrained and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "if not \"impact_label\" in df.columns:\n",
    "    unique_vals = df[\"citations\"].nunique()\n",
    "    if unique_vals >= 3:\n",
    "        df[\"impact_label\"] = pd.qcut(df[\"citations\"].fillna(0), q=3,\n",
    "                                     labels=[\"Low\", \"Medium\", \"High\"], duplicates=\"drop\")\n",
    "    elif unique_vals == 2:\n",
    "        df[\"impact_label\"] = pd.cut(df[\"citations\"].fillna(0), bins=2,\n",
    "                                    labels=[\"Low\", \"High\"])\n",
    "    else:\n",
    "        df[\"impact_label\"] = \"Low\"\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df[\"impact_label\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# ‚úÖ Save trained model & encoder\n",
    "joblib.dump(clf, \"physics_paper_model.pkl\")\n",
    "joblib.dump(le, \"label_encoder.pkl\")\n",
    "\n",
    "print(\"‚úÖ Model retrained and saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a47679da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìò Title: A polynomial time and space heuristic algorithm for T-count\n",
      "üîÆ Predicted Future Potential: Low\n"
     ]
    }
   ],
   "source": [
    "search = arxiv.Search(id_list=[\"2006.12440\"])\n",
    "result = next(client.results(search))\n",
    "\n",
    "title, abstract, doi = result.title, result.summary, result.doi\n",
    "crossref_meta = get_crossref_data(doi)\n",
    "openalex_meta = get_openalex_metadata(doi)\n",
    "rnd = get_rnd_expenditure(\"USA\")\n",
    "# Embed\n",
    "emb = embedder.encode([abstract])\n",
    "num_feats = np.array([[openalex_meta[\"citations\"] or 0,\n",
    "                       crossref_meta[\"pub_year\"] or 0,\n",
    "                       rnd or 0]])\n",
    "\n",
    "X_new = np.hstack((emb, num_feats))\n",
    "clf = joblib.load(\"physics_paper_model.pkl\")\n",
    "le = joblib.load(\"label_encoder.pkl\")\n",
    "\n",
    "pred = le.inverse_transform(clf.predict(X_new))[0]\n",
    "print(f\"\\nüìò Title: {title}\")\n",
    "print(f\"üîÆ Predicted Future Potential: {pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6219ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
